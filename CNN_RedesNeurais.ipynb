{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNj38pFcKdQejHJd4kz+i+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianzin30/CNN-RedesNeurais/blob/main/CNN_RedesNeurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports e bibliotecas"
      ],
      "metadata": {
        "id": "cWiGaDv-PmNb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMyHaBKcCewR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "from keras import backend as k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregando e Preprocessando os Dados\n"
      ],
      "metadata": {
        "id": "UonxupckPtKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregamento dos dados\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Definição das dimensões da imagem\n",
        "img_rows, img_cols = 28, 28"
      ],
      "metadata": {
        "id": "5vRvd4gsVwh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregamento dos dados\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Definição das dimensões da imagem\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# Reformatando as imagens com base na configuração de canal\n",
        "#def format_data(x_train, x_test, img_rows, img_cols):\n",
        " #   if keras.backend.image_data_format() == 'channels_first':\n",
        "   #     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    #    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "     #   inpx = (1, img_rows, img_cols)\n",
        "    #else:\n",
        "     #   x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "      #  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "       # inpx = (img_rows, img_cols, 1)\n",
        "\n",
        "    #return x_train, x_test, inpx\n",
        "\n",
        "#format_data(x_train,x_test,img_rows,img_cols)\n",
        "\n",
        "# Reformatando as imagens com base na configuração de canal\n",
        "if k.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    inpx = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    inpx = (img_rows, img_cols, 1)\n",
        "\n",
        "# Convertendo os dados para float e normalizando\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n"
      ],
      "metadata": {
        "id": "cpzwaUj9yzeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformando os rótulos ( labels ) em categorias\n"
      ],
      "metadata": {
        "id": "a89hfh02PwxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "U9GUdmkZyz7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rede Convolucional Simples\n",
        "\n",
        "Primeiro teste simples sobre redes convolucionais. A arquitetura da rede inclui uma camada convolucional, uma de flattening e densas.\n",
        "\n",
        "## Arquitetura da Rede\n",
        "\n",
        "\n",
        "1. **Primeira Camada Convolucional**: Aplicamos 32 filtros de tamanho 3x3 com função de ativação ReLU.\n",
        "2. **Flatten**: Convertendo a saída da camada convolucional em um vetor 1D.\n",
        "3. **Primeira Camada Densa**: Contém 250 neurônios com função de ativação sigmoide.\n",
        "4. **Camada de Saída**: Contém 10 neurônios (um para cada classe) com função de ativação softmax."
      ],
      "metadata": {
        "id": "F6Fz2SQaOP36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mudança"
      ],
      "metadata": {
        "id": "bqXv67e5h4Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "OPYwf3sChr0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inpx2 = Input(shape=(inpx))\n",
        "layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx2)\n",
        "flattened = Flatten()(layer1)  # Add this line to flatten the output of the Conv2D layer\n",
        "layer2 = Dense(250, activation='sigmoid')(flattened)\n",
        "layer3 = Dense(10, activation='softmax')(layer2)\n",
        "\n"
      ],
      "metadata": {
        "id": "lZ_JclQEy1qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([inpx2], layer3)\n",
        "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
        "\t\t\tloss=keras.losses.categorical_crossentropy,\n",
        "\t\t\tmetrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=240, batch_size=500)"
      ],
      "metadata": {
        "id": "tWz7-fWz6WMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327983b8-710e-4e79-92ed-87d98ef8fc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "120/120 [==============================] - 2s 10ms/step - loss: 2.5797 - accuracy: 0.0904\n",
            "Epoch 2/2\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.5402 - accuracy: 0.0903\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7962b6d73610>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('loss=', score[0])\n",
        "print('accuracy=', score[1])\n"
      ],
      "metadata": {
        "id": "46XpVSGp6YCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232d610b-e85a-4f65-b206-e84399f06019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss= 0.31196504831314087\n",
            "accuracy= 0.9154000282287598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def treinamento(primeiro_layer,epocas,ultimo_layer):\n",
        "  model = Model([inpx], ultimo_layer)\n",
        "  model.compile(optimizer=keras.optimizers.Adadelta(),\n",
        "        loss=keras.losses.categorical_crossentropy,\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "  model.fit(x_train, y_train, epochs=epocas, batch_size=500)"
      ],
      "metadata": {
        "id": "mRYNPdmQTzsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stats(x):\n",
        "  score = x.evaluate(x_test, y_test, verbose=0)\n",
        "  print('loss=', score[0])\n",
        "  print('accuracy=', score[1])"
      ],
      "metadata": {
        "id": "7_4kU4yuUApW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes com diferentes tamanhos de filtro"
      ],
      "metadata": {
        "id": "BOGtUmB0SDED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teste 1 (5x5)"
      ],
      "metadata": {
        "id": "JPDsIK4KSUe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teste1 = Input(shape=(inpx))\n",
        "layer1 = Conv2D(32, kernel_size=(5, 5), activation='relu')(teste1)\n",
        "flattened = Flatten()(layer1)  # Add this line to flatten the output of the Conv2D layer\n",
        "layer2 = Dense(250, activation='sigmoid')(flattened)\n",
        "layer3 = Dense(10, activation='softmax')(layer2)"
      ],
      "metadata": {
        "id": "BUU2doZ6SSX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([teste1], layer3)\n",
        "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
        "\t\t\tloss=keras.losses.categorical_crossentropy,\n",
        "\t\t\tmetrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=240, batch_size=500)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3LO8bKqTH3B",
        "outputId": "89efbcbd-134d-4e1e-fe83-be4be71dce12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/240\n",
            "120/120 [==============================] - 3s 12ms/step - loss: 2.4336 - accuracy: 0.0975\n",
            "Epoch 2/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.4048 - accuracy: 0.0976\n",
            "Epoch 3/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.3769 - accuracy: 0.0988\n",
            "Epoch 4/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.3494 - accuracy: 0.1036\n",
            "Epoch 5/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.3222 - accuracy: 0.1138\n",
            "Epoch 6/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2951 - accuracy: 0.1280\n",
            "Epoch 7/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2680 - accuracy: 0.1443\n",
            "Epoch 8/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.2408 - accuracy: 0.1719\n",
            "Epoch 9/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.2133 - accuracy: 0.2197\n",
            "Epoch 10/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.1853 - accuracy: 0.2923\n",
            "Epoch 11/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.1566 - accuracy: 0.3823\n",
            "Epoch 12/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.1267 - accuracy: 0.4755\n",
            "Epoch 13/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.0954 - accuracy: 0.5447\n",
            "Epoch 14/240\n",
            "120/120 [==============================] - 2s 16ms/step - loss: 2.0624 - accuracy: 0.5934\n",
            "Epoch 15/240\n",
            "120/120 [==============================] - 2s 15ms/step - loss: 2.0272 - accuracy: 0.6248\n",
            "Epoch 16/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 1.9895 - accuracy: 0.6480\n",
            "Epoch 17/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 1.9493 - accuracy: 0.6648\n",
            "Epoch 18/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 1.9064 - accuracy: 0.6803\n",
            "Epoch 19/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 1.8608 - accuracy: 0.6922\n",
            "Epoch 20/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.8127 - accuracy: 0.7039\n",
            "Epoch 21/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.7624 - accuracy: 0.7137\n",
            "Epoch 22/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.7103 - accuracy: 0.7224\n",
            "Epoch 23/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.6569 - accuracy: 0.7295\n",
            "Epoch 24/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.6028 - accuracy: 0.7365\n",
            "Epoch 25/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.5485 - accuracy: 0.7427\n",
            "Epoch 26/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 1.4947 - accuracy: 0.7488\n",
            "Epoch 27/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.4418 - accuracy: 0.7551\n",
            "Epoch 28/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.3905 - accuracy: 0.7607\n",
            "Epoch 29/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.3410 - accuracy: 0.7654\n",
            "Epoch 30/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.2937 - accuracy: 0.7712\n",
            "Epoch 31/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 1.2486 - accuracy: 0.7760\n",
            "Epoch 32/240\n",
            "120/120 [==============================] - 2s 14ms/step - loss: 1.2060 - accuracy: 0.7808\n",
            "Epoch 33/240\n",
            "120/120 [==============================] - 2s 16ms/step - loss: 1.1657 - accuracy: 0.7859\n",
            "Epoch 34/240\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 1.1279 - accuracy: 0.7903\n",
            "Epoch 35/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 1.0925 - accuracy: 0.7944\n",
            "Epoch 36/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.0593 - accuracy: 0.7990\n",
            "Epoch 37/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.0284 - accuracy: 0.8032\n",
            "Epoch 38/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.9994 - accuracy: 0.8063\n",
            "Epoch 39/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.9724 - accuracy: 0.8099\n",
            "Epoch 40/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.9471 - accuracy: 0.8133\n",
            "Epoch 41/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.9235 - accuracy: 0.8164\n",
            "Epoch 42/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.9012 - accuracy: 0.8194\n",
            "Epoch 43/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8804 - accuracy: 0.8220\n",
            "Epoch 44/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8608 - accuracy: 0.8245\n",
            "Epoch 45/240\n",
            "120/120 [==============================] - 2s 14ms/step - loss: 0.8424 - accuracy: 0.8270\n",
            "Epoch 46/240\n",
            "120/120 [==============================] - 2s 13ms/step - loss: 0.8251 - accuracy: 0.8291\n",
            "Epoch 47/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.8088 - accuracy: 0.8313\n",
            "Epoch 48/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7934 - accuracy: 0.8330\n",
            "Epoch 49/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7788 - accuracy: 0.8354\n",
            "Epoch 50/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7651 - accuracy: 0.8371\n",
            "Epoch 51/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7520 - accuracy: 0.8387\n",
            "Epoch 52/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7397 - accuracy: 0.8407\n",
            "Epoch 53/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.7280 - accuracy: 0.8420\n",
            "Epoch 54/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7168 - accuracy: 0.8435\n",
            "Epoch 55/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7062 - accuracy: 0.8453\n",
            "Epoch 56/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6961 - accuracy: 0.8464\n",
            "Epoch 57/240\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6865 - accuracy: 0.8478\n",
            "Epoch 58/240\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6773 - accuracy: 0.8493\n",
            "Epoch 59/240\n",
            "120/120 [==============================] - 2s 21ms/step - loss: 0.6685 - accuracy: 0.8503\n",
            "Epoch 60/240\n",
            "120/120 [==============================] - 2s 15ms/step - loss: 0.6601 - accuracy: 0.8515\n",
            "Epoch 61/240\n",
            "120/120 [==============================] - 2s 14ms/step - loss: 0.6520 - accuracy: 0.8528\n",
            "Epoch 62/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.6443 - accuracy: 0.8537\n",
            "Epoch 63/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.6368 - accuracy: 0.8547\n",
            "Epoch 64/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6297 - accuracy: 0.8557\n",
            "Epoch 65/240\n",
            "120/120 [==============================] - 2s 15ms/step - loss: 0.6228 - accuracy: 0.8569\n",
            "Epoch 66/240\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 0.6162 - accuracy: 0.8581\n",
            "Epoch 67/240\n",
            "120/120 [==============================] - 2s 13ms/step - loss: 0.6099 - accuracy: 0.8590\n",
            "Epoch 68/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6038 - accuracy: 0.8599\n",
            "Epoch 69/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5979 - accuracy: 0.8608\n",
            "Epoch 70/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5922 - accuracy: 0.8617\n",
            "Epoch 71/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5867 - accuracy: 0.8622\n",
            "Epoch 72/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5813 - accuracy: 0.8630\n",
            "Epoch 73/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5762 - accuracy: 0.8638\n",
            "Epoch 74/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5712 - accuracy: 0.8647\n",
            "Epoch 75/240\n",
            "120/120 [==============================] - 2s 15ms/step - loss: 0.5664 - accuracy: 0.8655\n",
            "Epoch 76/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5617 - accuracy: 0.8663\n",
            "Epoch 77/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5572 - accuracy: 0.8669\n",
            "Epoch 78/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5528 - accuracy: 0.8675\n",
            "Epoch 79/240\n",
            "120/120 [==============================] - 2s 16ms/step - loss: 0.5485 - accuracy: 0.8681\n",
            "Epoch 80/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.5444 - accuracy: 0.8686\n",
            "Epoch 81/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.5404 - accuracy: 0.8691\n",
            "Epoch 82/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5365 - accuracy: 0.8697\n",
            "Epoch 83/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5327 - accuracy: 0.8701\n",
            "Epoch 84/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5290 - accuracy: 0.8706\n",
            "Epoch 85/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5254 - accuracy: 0.8716\n",
            "Epoch 86/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5219 - accuracy: 0.8719\n",
            "Epoch 87/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5184 - accuracy: 0.8726\n",
            "Epoch 88/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5151 - accuracy: 0.8735\n",
            "Epoch 89/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5119 - accuracy: 0.8740\n",
            "Epoch 90/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5087 - accuracy: 0.8745\n",
            "Epoch 91/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5056 - accuracy: 0.8750\n",
            "Epoch 92/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5026 - accuracy: 0.8756\n",
            "Epoch 93/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4996 - accuracy: 0.8762\n",
            "Epoch 94/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4967 - accuracy: 0.8769\n",
            "Epoch 95/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4939 - accuracy: 0.8770\n",
            "Epoch 96/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4911 - accuracy: 0.8777\n",
            "Epoch 97/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4884 - accuracy: 0.8780\n",
            "Epoch 98/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4858 - accuracy: 0.8784\n",
            "Epoch 99/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4832 - accuracy: 0.8791\n",
            "Epoch 100/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4806 - accuracy: 0.8796\n",
            "Epoch 101/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4782 - accuracy: 0.8798\n",
            "Epoch 102/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4757 - accuracy: 0.8806\n",
            "Epoch 103/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4733 - accuracy: 0.8808\n",
            "Epoch 104/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4710 - accuracy: 0.8813\n",
            "Epoch 105/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4687 - accuracy: 0.8818\n",
            "Epoch 106/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4665 - accuracy: 0.8820\n",
            "Epoch 107/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4643 - accuracy: 0.8827\n",
            "Epoch 108/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4621 - accuracy: 0.8831\n",
            "Epoch 109/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.4600 - accuracy: 0.8836\n",
            "Epoch 110/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4579 - accuracy: 0.8839\n",
            "Epoch 111/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4559 - accuracy: 0.8842\n",
            "Epoch 112/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4539 - accuracy: 0.8844\n",
            "Epoch 113/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4519 - accuracy: 0.8848\n",
            "Epoch 114/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4500 - accuracy: 0.8856\n",
            "Epoch 115/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4481 - accuracy: 0.8856\n",
            "Epoch 116/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4462 - accuracy: 0.8860\n",
            "Epoch 117/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4443 - accuracy: 0.8862\n",
            "Epoch 118/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4425 - accuracy: 0.8865\n",
            "Epoch 119/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4407 - accuracy: 0.8869\n",
            "Epoch 120/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4390 - accuracy: 0.8872\n",
            "Epoch 121/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4373 - accuracy: 0.8876\n",
            "Epoch 122/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4356 - accuracy: 0.8879\n",
            "Epoch 123/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4339 - accuracy: 0.8881\n",
            "Epoch 124/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4322 - accuracy: 0.8886\n",
            "Epoch 125/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4306 - accuracy: 0.8889\n",
            "Epoch 126/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4290 - accuracy: 0.8891\n",
            "Epoch 127/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4275 - accuracy: 0.8894\n",
            "Epoch 128/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4259 - accuracy: 0.8898\n",
            "Epoch 129/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4244 - accuracy: 0.8900\n",
            "Epoch 130/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4229 - accuracy: 0.8900\n",
            "Epoch 131/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4214 - accuracy: 0.8902\n",
            "Epoch 132/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4200 - accuracy: 0.8905\n",
            "Epoch 133/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4185 - accuracy: 0.8907\n",
            "Epoch 134/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4171 - accuracy: 0.8907\n",
            "Epoch 135/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4157 - accuracy: 0.8910\n",
            "Epoch 136/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4143 - accuracy: 0.8913\n",
            "Epoch 137/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.4130 - accuracy: 0.8915\n",
            "Epoch 138/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.4116 - accuracy: 0.8917\n",
            "Epoch 139/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4103 - accuracy: 0.8920\n",
            "Epoch 140/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4090 - accuracy: 0.8922\n",
            "Epoch 141/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4077 - accuracy: 0.8927\n",
            "Epoch 142/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4064 - accuracy: 0.8928\n",
            "Epoch 143/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4051 - accuracy: 0.8931\n",
            "Epoch 144/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4039 - accuracy: 0.8933\n",
            "Epoch 145/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4027 - accuracy: 0.8937\n",
            "Epoch 146/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4015 - accuracy: 0.8939\n",
            "Epoch 147/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4003 - accuracy: 0.8941\n",
            "Epoch 148/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3991 - accuracy: 0.8942\n",
            "Epoch 149/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3979 - accuracy: 0.8945\n",
            "Epoch 150/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3967 - accuracy: 0.8948\n",
            "Epoch 151/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3956 - accuracy: 0.8950\n",
            "Epoch 152/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3945 - accuracy: 0.8953\n",
            "Epoch 153/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3933 - accuracy: 0.8954\n",
            "Epoch 154/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3922 - accuracy: 0.8958\n",
            "Epoch 155/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3911 - accuracy: 0.8960\n",
            "Epoch 156/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3901 - accuracy: 0.8963\n",
            "Epoch 157/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3890 - accuracy: 0.8965\n",
            "Epoch 158/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3879 - accuracy: 0.8967\n",
            "Epoch 159/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3869 - accuracy: 0.8970\n",
            "Epoch 160/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3859 - accuracy: 0.8970\n",
            "Epoch 161/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3848 - accuracy: 0.8974\n",
            "Epoch 162/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3838 - accuracy: 0.8975\n",
            "Epoch 163/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3828 - accuracy: 0.8977\n",
            "Epoch 164/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3818 - accuracy: 0.8979\n",
            "Epoch 165/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3809 - accuracy: 0.8982\n",
            "Epoch 166/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3799 - accuracy: 0.8985\n",
            "Epoch 167/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3789 - accuracy: 0.8986\n",
            "Epoch 168/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.3780 - accuracy: 0.8990\n",
            "Epoch 169/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3770 - accuracy: 0.8991\n",
            "Epoch 170/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3761 - accuracy: 0.8989\n",
            "Epoch 171/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.3752 - accuracy: 0.8993\n",
            "Epoch 172/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3742 - accuracy: 0.8995\n",
            "Epoch 173/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3733 - accuracy: 0.8997\n",
            "Epoch 174/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3724 - accuracy: 0.8998\n",
            "Epoch 175/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3715 - accuracy: 0.9001\n",
            "Epoch 176/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3706 - accuracy: 0.9003\n",
            "Epoch 177/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3698 - accuracy: 0.9005\n",
            "Epoch 178/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3689 - accuracy: 0.9006\n",
            "Epoch 179/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3680 - accuracy: 0.9007\n",
            "Epoch 180/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3672 - accuracy: 0.9009\n",
            "Epoch 181/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3664 - accuracy: 0.9010\n",
            "Epoch 182/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3655 - accuracy: 0.9010\n",
            "Epoch 183/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3647 - accuracy: 0.9015\n",
            "Epoch 184/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3639 - accuracy: 0.9015\n",
            "Epoch 185/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3631 - accuracy: 0.9017\n",
            "Epoch 186/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3623 - accuracy: 0.9018\n",
            "Epoch 187/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3615 - accuracy: 0.9021\n",
            "Epoch 188/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3607 - accuracy: 0.9022\n",
            "Epoch 189/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3599 - accuracy: 0.9024\n",
            "Epoch 190/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3591 - accuracy: 0.9026\n",
            "Epoch 191/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3583 - accuracy: 0.9027\n",
            "Epoch 192/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3575 - accuracy: 0.9030\n",
            "Epoch 193/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3568 - accuracy: 0.9032\n",
            "Epoch 194/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3560 - accuracy: 0.9034\n",
            "Epoch 195/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3553 - accuracy: 0.9035\n",
            "Epoch 196/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3545 - accuracy: 0.9034\n",
            "Epoch 197/240\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 0.3538 - accuracy: 0.9038\n",
            "Epoch 198/240\n",
            "120/120 [==============================] - 2s 14ms/step - loss: 0.3531 - accuracy: 0.9039\n",
            "Epoch 199/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3523 - accuracy: 0.9042\n",
            "Epoch 200/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3516 - accuracy: 0.9044\n",
            "Epoch 201/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3509 - accuracy: 0.9046\n",
            "Epoch 202/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3502 - accuracy: 0.9047\n",
            "Epoch 203/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3495 - accuracy: 0.9050\n",
            "Epoch 204/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3488 - accuracy: 0.9051\n",
            "Epoch 205/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3481 - accuracy: 0.9054\n",
            "Epoch 206/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3474 - accuracy: 0.9054\n",
            "Epoch 207/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3467 - accuracy: 0.9055\n",
            "Epoch 208/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3460 - accuracy: 0.9055\n",
            "Epoch 209/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3454 - accuracy: 0.9057\n",
            "Epoch 210/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3447 - accuracy: 0.9059\n",
            "Epoch 211/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3440 - accuracy: 0.9060\n",
            "Epoch 212/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3434 - accuracy: 0.9063\n",
            "Epoch 213/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3427 - accuracy: 0.9064\n",
            "Epoch 214/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3421 - accuracy: 0.9065\n",
            "Epoch 215/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3414 - accuracy: 0.9067\n",
            "Epoch 216/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3408 - accuracy: 0.9068\n",
            "Epoch 217/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3401 - accuracy: 0.9070\n",
            "Epoch 218/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3395 - accuracy: 0.9071\n",
            "Epoch 219/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3389 - accuracy: 0.9071\n",
            "Epoch 220/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3382 - accuracy: 0.9074\n",
            "Epoch 221/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3376 - accuracy: 0.9074\n",
            "Epoch 222/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3370 - accuracy: 0.9074\n",
            "Epoch 223/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3364 - accuracy: 0.9076\n",
            "Epoch 224/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3358 - accuracy: 0.9077\n",
            "Epoch 225/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3352 - accuracy: 0.9078\n",
            "Epoch 226/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.3346 - accuracy: 0.9080\n",
            "Epoch 227/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3340 - accuracy: 0.9082\n",
            "Epoch 228/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3334 - accuracy: 0.9082\n",
            "Epoch 229/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3328 - accuracy: 0.9084\n",
            "Epoch 230/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3322 - accuracy: 0.9086\n",
            "Epoch 231/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3316 - accuracy: 0.9086\n",
            "Epoch 232/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3311 - accuracy: 0.9088\n",
            "Epoch 233/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.3305 - accuracy: 0.9090\n",
            "Epoch 234/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.3299 - accuracy: 0.9091\n",
            "Epoch 235/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3293 - accuracy: 0.9093\n",
            "Epoch 236/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3288 - accuracy: 0.9094\n",
            "Epoch 237/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3282 - accuracy: 0.9095\n",
            "Epoch 238/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3276 - accuracy: 0.9098\n",
            "Epoch 239/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3271 - accuracy: 0.9096\n",
            "Epoch 240/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3265 - accuracy: 0.9098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats(model)"
      ],
      "metadata": {
        "id": "9iSpowFnUGWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5839c01-333c-4179-e234-2ac0795908ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss= 0.31196504831314087\n",
            "accuracy= 0.9154000282287598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 7x7\n"
      ],
      "metadata": {
        "id": "1E_DP7JwcyGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teste7x7 = Input(shape=(inpx))\n",
        "layer1 = Conv2D(32, kernel_size=(7, 7), activation='relu')(teste7x7)\n",
        "flattened = Flatten()(layer1)  # Add this line to flatten the output of the Conv2D layer\n",
        "layer2 = Dense(250, activation='sigmoid')(flattened)\n",
        "layer3 = Dense(10, activation='softmax')(layer2)"
      ],
      "metadata": {
        "id": "lp-PlFVhc31r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model7x7 = Model([teste7x7], layer3)\n",
        "model7x7.compile(optimizer=keras.optimizers.Adadelta(),\n",
        "\t\t\tloss=keras.losses.categorical_crossentropy,\n",
        "\t\t\tmetrics=['accuracy'])\n",
        "\n",
        "model7x7.fit(x_train, y_train, epochs=240, batch_size=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vnGocHHc8LQ",
        "outputId": "88845300-ac37-4225-914d-46697bf6f892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/240\n",
            "120/120 [==============================] - 2s 9ms/step - loss: 2.4876 - accuracy: 0.0975\n",
            "Epoch 2/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.4448 - accuracy: 0.0975\n",
            "Epoch 3/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.4035 - accuracy: 0.0988\n",
            "Epoch 4/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.3629 - accuracy: 0.1054\n",
            "Epoch 5/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.3229 - accuracy: 0.1195\n",
            "Epoch 6/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2837 - accuracy: 0.1368\n",
            "Epoch 7/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2458 - accuracy: 0.1529\n",
            "Epoch 8/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2093 - accuracy: 0.1798\n",
            "Epoch 9/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 2.1742 - accuracy: 0.2485\n",
            "Epoch 10/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.1400 - accuracy: 0.3476\n",
            "Epoch 11/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.1060 - accuracy: 0.4412\n",
            "Epoch 12/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.0715 - accuracy: 0.5068\n",
            "Epoch 13/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.0358 - accuracy: 0.5538\n",
            "Epoch 14/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.9984 - accuracy: 0.5867\n",
            "Epoch 15/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.9587 - accuracy: 0.6119\n",
            "Epoch 16/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.9166 - accuracy: 0.6325\n",
            "Epoch 17/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.8720 - accuracy: 0.6517\n",
            "Epoch 18/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.8251 - accuracy: 0.6674\n",
            "Epoch 19/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.7759 - accuracy: 0.6818\n",
            "Epoch 20/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.7249 - accuracy: 0.6959\n",
            "Epoch 21/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 1.6725 - accuracy: 0.7088\n",
            "Epoch 22/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 1.6191 - accuracy: 0.7191\n",
            "Epoch 23/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.5653 - accuracy: 0.7303\n",
            "Epoch 24/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.5117 - accuracy: 0.7387\n",
            "Epoch 25/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.4587 - accuracy: 0.7462\n",
            "Epoch 26/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.4068 - accuracy: 0.7534\n",
            "Epoch 27/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.3566 - accuracy: 0.7602\n",
            "Epoch 28/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.3083 - accuracy: 0.7664\n",
            "Epoch 29/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.2622 - accuracy: 0.7724\n",
            "Epoch 30/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.2185 - accuracy: 0.7773\n",
            "Epoch 31/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.1771 - accuracy: 0.7826\n",
            "Epoch 32/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.1383 - accuracy: 0.7877\n",
            "Epoch 33/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.1019 - accuracy: 0.7914\n",
            "Epoch 34/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.0679 - accuracy: 0.7953\n",
            "Epoch 35/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 1.0361 - accuracy: 0.7999\n",
            "Epoch 36/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.0064 - accuracy: 0.8031\n",
            "Epoch 37/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.9786 - accuracy: 0.8067\n",
            "Epoch 38/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.9527 - accuracy: 0.8102\n",
            "Epoch 39/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.9285 - accuracy: 0.8133\n",
            "Epoch 40/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.9059 - accuracy: 0.8163\n",
            "Epoch 41/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8847 - accuracy: 0.8194\n",
            "Epoch 42/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8649 - accuracy: 0.8221\n",
            "Epoch 43/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8462 - accuracy: 0.8253\n",
            "Epoch 44/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8287 - accuracy: 0.8272\n",
            "Epoch 45/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.8122 - accuracy: 0.8292\n",
            "Epoch 46/240\n",
            "120/120 [==============================] - 2s 16ms/step - loss: 0.7966 - accuracy: 0.8317\n",
            "Epoch 47/240\n",
            "120/120 [==============================] - 2s 14ms/step - loss: 0.7819 - accuracy: 0.8333\n",
            "Epoch 48/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7680 - accuracy: 0.8351\n",
            "Epoch 49/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7549 - accuracy: 0.8369\n",
            "Epoch 50/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7424 - accuracy: 0.8386\n",
            "Epoch 51/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7306 - accuracy: 0.8399\n",
            "Epoch 52/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7193 - accuracy: 0.8411\n",
            "Epoch 53/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7086 - accuracy: 0.8426\n",
            "Epoch 54/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6984 - accuracy: 0.8439\n",
            "Epoch 55/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.6887 - accuracy: 0.8458\n",
            "Epoch 56/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.6794 - accuracy: 0.8468\n",
            "Epoch 57/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.6705 - accuracy: 0.8482\n",
            "Epoch 58/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6620 - accuracy: 0.8493\n",
            "Epoch 59/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6539 - accuracy: 0.8507\n",
            "Epoch 60/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6461 - accuracy: 0.8517\n",
            "Epoch 61/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.6386 - accuracy: 0.8528\n",
            "Epoch 62/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6314 - accuracy: 0.8543\n",
            "Epoch 63/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.6244 - accuracy: 0.8549\n",
            "Epoch 64/240\n",
            "120/120 [==============================] - 2s 13ms/step - loss: 0.6178 - accuracy: 0.8562\n",
            "Epoch 65/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6114 - accuracy: 0.8574\n",
            "Epoch 66/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6052 - accuracy: 0.8583\n",
            "Epoch 67/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5992 - accuracy: 0.8590\n",
            "Epoch 68/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5934 - accuracy: 0.8600\n",
            "Epoch 69/240\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.5879 - accuracy: 0.8608\n",
            "Epoch 70/240\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.5825 - accuracy: 0.8620\n",
            "Epoch 71/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5773 - accuracy: 0.8627\n",
            "Epoch 72/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5723 - accuracy: 0.8633\n",
            "Epoch 73/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5674 - accuracy: 0.8640\n",
            "Epoch 74/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5627 - accuracy: 0.8647\n",
            "Epoch 75/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5581 - accuracy: 0.8656\n",
            "Epoch 76/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5537 - accuracy: 0.8665\n",
            "Epoch 77/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5494 - accuracy: 0.8670\n",
            "Epoch 78/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5452 - accuracy: 0.8676\n",
            "Epoch 79/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5411 - accuracy: 0.8684\n",
            "Epoch 80/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5371 - accuracy: 0.8691\n",
            "Epoch 81/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5333 - accuracy: 0.8697\n",
            "Epoch 82/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5296 - accuracy: 0.8701\n",
            "Epoch 83/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5259 - accuracy: 0.8708\n",
            "Epoch 84/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.5224 - accuracy: 0.8713\n",
            "Epoch 85/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5189 - accuracy: 0.8719\n",
            "Epoch 86/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5155 - accuracy: 0.8726\n",
            "Epoch 87/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5122 - accuracy: 0.8731\n",
            "Epoch 88/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5090 - accuracy: 0.8736\n",
            "Epoch 89/240\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.5059 - accuracy: 0.8741\n",
            "Epoch 90/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5028 - accuracy: 0.8750\n",
            "Epoch 91/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4999 - accuracy: 0.8753\n",
            "Epoch 92/240\n",
            "120/120 [==============================] - 2s 14ms/step - loss: 0.4969 - accuracy: 0.8756\n",
            "Epoch 93/240\n",
            "120/120 [==============================] - 2s 16ms/step - loss: 0.4941 - accuracy: 0.8761\n",
            "Epoch 94/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4913 - accuracy: 0.8768\n",
            "Epoch 95/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4885 - accuracy: 0.8773\n",
            "Epoch 96/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4859 - accuracy: 0.8778\n",
            "Epoch 97/240\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.4833 - accuracy: 0.8784\n",
            "Epoch 98/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4807 - accuracy: 0.8790\n",
            "Epoch 99/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4782 - accuracy: 0.8794\n",
            "Epoch 100/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4758 - accuracy: 0.8800\n",
            "Epoch 101/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4734 - accuracy: 0.8806\n",
            "Epoch 102/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4710 - accuracy: 0.8809\n",
            "Epoch 103/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4687 - accuracy: 0.8815\n",
            "Epoch 104/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4665 - accuracy: 0.8817\n",
            "Epoch 105/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4643 - accuracy: 0.8821\n",
            "Epoch 106/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4621 - accuracy: 0.8825\n",
            "Epoch 107/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4600 - accuracy: 0.8829\n",
            "Epoch 108/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4579 - accuracy: 0.8832\n",
            "Epoch 109/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4558 - accuracy: 0.8834\n",
            "Epoch 110/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4538 - accuracy: 0.8838\n",
            "Epoch 111/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4518 - accuracy: 0.8842\n",
            "Epoch 112/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4499 - accuracy: 0.8845\n",
            "Epoch 113/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4480 - accuracy: 0.8849\n",
            "Epoch 114/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4461 - accuracy: 0.8850\n",
            "Epoch 115/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4442 - accuracy: 0.8856\n",
            "Epoch 116/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4424 - accuracy: 0.8856\n",
            "Epoch 117/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4406 - accuracy: 0.8863\n",
            "Epoch 118/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4389 - accuracy: 0.8866\n",
            "Epoch 119/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4371 - accuracy: 0.8866\n",
            "Epoch 120/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4354 - accuracy: 0.8875\n",
            "Epoch 121/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4337 - accuracy: 0.8877\n",
            "Epoch 122/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4321 - accuracy: 0.8879\n",
            "Epoch 123/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4305 - accuracy: 0.8881\n",
            "Epoch 124/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4289 - accuracy: 0.8886\n",
            "Epoch 125/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4273 - accuracy: 0.8886\n",
            "Epoch 126/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4257 - accuracy: 0.8891\n",
            "Epoch 127/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4242 - accuracy: 0.8894\n",
            "Epoch 128/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4227 - accuracy: 0.8896\n",
            "Epoch 129/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4212 - accuracy: 0.8900\n",
            "Epoch 130/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4198 - accuracy: 0.8904\n",
            "Epoch 131/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4183 - accuracy: 0.8904\n",
            "Epoch 132/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4169 - accuracy: 0.8908\n",
            "Epoch 133/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4155 - accuracy: 0.8909\n",
            "Epoch 134/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4141 - accuracy: 0.8914\n",
            "Epoch 135/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4127 - accuracy: 0.8915\n",
            "Epoch 136/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4114 - accuracy: 0.8918\n",
            "Epoch 137/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4101 - accuracy: 0.8920\n",
            "Epoch 138/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4088 - accuracy: 0.8924\n",
            "Epoch 139/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4075 - accuracy: 0.8924\n",
            "Epoch 140/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4062 - accuracy: 0.8928\n",
            "Epoch 141/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4049 - accuracy: 0.8929\n",
            "Epoch 142/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4037 - accuracy: 0.8931\n",
            "Epoch 143/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4025 - accuracy: 0.8932\n",
            "Epoch 144/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4012 - accuracy: 0.8935\n",
            "Epoch 145/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4000 - accuracy: 0.8936\n",
            "Epoch 146/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3989 - accuracy: 0.8939\n",
            "Epoch 147/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3977 - accuracy: 0.8942\n",
            "Epoch 148/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3965 - accuracy: 0.8943\n",
            "Epoch 149/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3954 - accuracy: 0.8946\n",
            "Epoch 150/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3943 - accuracy: 0.8948\n",
            "Epoch 151/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3931 - accuracy: 0.8950\n",
            "Epoch 152/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3920 - accuracy: 0.8953\n",
            "Epoch 153/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3909 - accuracy: 0.8956\n",
            "Epoch 154/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3899 - accuracy: 0.8958\n",
            "Epoch 155/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3888 - accuracy: 0.8961\n",
            "Epoch 156/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3877 - accuracy: 0.8963\n",
            "Epoch 157/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3867 - accuracy: 0.8966\n",
            "Epoch 158/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3857 - accuracy: 0.8969\n",
            "Epoch 159/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3846 - accuracy: 0.8971\n",
            "Epoch 160/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3836 - accuracy: 0.8972\n",
            "Epoch 161/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3826 - accuracy: 0.8974\n",
            "Epoch 162/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3816 - accuracy: 0.8977\n",
            "Epoch 163/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3806 - accuracy: 0.8978\n",
            "Epoch 164/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3797 - accuracy: 0.8981\n",
            "Epoch 165/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3787 - accuracy: 0.8982\n",
            "Epoch 166/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3777 - accuracy: 0.8984\n",
            "Epoch 167/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3768 - accuracy: 0.8984\n",
            "Epoch 168/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3759 - accuracy: 0.8987\n",
            "Epoch 169/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3749 - accuracy: 0.8988\n",
            "Epoch 170/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3740 - accuracy: 0.8988\n",
            "Epoch 171/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3731 - accuracy: 0.8991\n",
            "Epoch 172/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3722 - accuracy: 0.8991\n",
            "Epoch 173/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3713 - accuracy: 0.8995\n",
            "Epoch 174/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3704 - accuracy: 0.8997\n",
            "Epoch 175/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3695 - accuracy: 0.8997\n",
            "Epoch 176/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3687 - accuracy: 0.9001\n",
            "Epoch 177/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3678 - accuracy: 0.9002\n",
            "Epoch 178/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3669 - accuracy: 0.9003\n",
            "Epoch 179/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3661 - accuracy: 0.9006\n",
            "Epoch 180/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3652 - accuracy: 0.9008\n",
            "Epoch 181/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3644 - accuracy: 0.9008\n",
            "Epoch 182/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3636 - accuracy: 0.9009\n",
            "Epoch 183/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3628 - accuracy: 0.9012\n",
            "Epoch 184/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3620 - accuracy: 0.9014\n",
            "Epoch 185/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3611 - accuracy: 0.9015\n",
            "Epoch 186/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3603 - accuracy: 0.9017\n",
            "Epoch 187/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3596 - accuracy: 0.9019\n",
            "Epoch 188/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3588 - accuracy: 0.9021\n",
            "Epoch 189/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3580 - accuracy: 0.9022\n",
            "Epoch 190/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3572 - accuracy: 0.9024\n",
            "Epoch 191/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3564 - accuracy: 0.9025\n",
            "Epoch 192/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3557 - accuracy: 0.9028\n",
            "Epoch 193/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3549 - accuracy: 0.9028\n",
            "Epoch 194/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3542 - accuracy: 0.9030\n",
            "Epoch 195/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3535 - accuracy: 0.9032\n",
            "Epoch 196/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3527 - accuracy: 0.9034\n",
            "Epoch 197/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3520 - accuracy: 0.9035\n",
            "Epoch 198/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3513 - accuracy: 0.9038\n",
            "Epoch 199/240\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3505 - accuracy: 0.9041\n",
            "Epoch 200/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3498 - accuracy: 0.9042\n",
            "Epoch 201/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3491 - accuracy: 0.9042\n",
            "Epoch 202/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3484 - accuracy: 0.9046\n",
            "Epoch 203/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3477 - accuracy: 0.9048\n",
            "Epoch 204/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3470 - accuracy: 0.9049\n",
            "Epoch 205/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3463 - accuracy: 0.9050\n",
            "Epoch 206/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3456 - accuracy: 0.9052\n",
            "Epoch 207/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3450 - accuracy: 0.9053\n",
            "Epoch 208/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3443 - accuracy: 0.9054\n",
            "Epoch 209/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3436 - accuracy: 0.9056\n",
            "Epoch 210/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3430 - accuracy: 0.9057\n",
            "Epoch 211/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3423 - accuracy: 0.9058\n",
            "Epoch 212/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3416 - accuracy: 0.9060\n",
            "Epoch 213/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3410 - accuracy: 0.9062\n",
            "Epoch 214/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3403 - accuracy: 0.9063\n",
            "Epoch 215/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3397 - accuracy: 0.9065\n",
            "Epoch 216/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3390 - accuracy: 0.9068\n",
            "Epoch 217/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3384 - accuracy: 0.9068\n",
            "Epoch 218/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3378 - accuracy: 0.9068\n",
            "Epoch 219/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3371 - accuracy: 0.9071\n",
            "Epoch 220/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3365 - accuracy: 0.9072\n",
            "Epoch 221/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3359 - accuracy: 0.9074\n",
            "Epoch 222/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3353 - accuracy: 0.9075\n",
            "Epoch 223/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3347 - accuracy: 0.9075\n",
            "Epoch 224/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3341 - accuracy: 0.9078\n",
            "Epoch 225/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3335 - accuracy: 0.9081\n",
            "Epoch 226/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3329 - accuracy: 0.9081\n",
            "Epoch 227/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3323 - accuracy: 0.9082\n",
            "Epoch 228/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3317 - accuracy: 0.9084\n",
            "Epoch 229/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3311 - accuracy: 0.9088\n",
            "Epoch 230/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3305 - accuracy: 0.9089\n",
            "Epoch 231/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3299 - accuracy: 0.9089\n",
            "Epoch 232/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3293 - accuracy: 0.9092\n",
            "Epoch 233/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3287 - accuracy: 0.9094\n",
            "Epoch 234/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3282 - accuracy: 0.9093\n",
            "Epoch 235/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3276 - accuracy: 0.9098\n",
            "Epoch 236/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3270 - accuracy: 0.9098\n",
            "Epoch 237/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3265 - accuracy: 0.9100\n",
            "Epoch 238/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3259 - accuracy: 0.9102\n",
            "Epoch 239/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3254 - accuracy: 0.9104\n",
            "Epoch 240/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3248 - accuracy: 0.9105\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x796342bed900>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats(model7x7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvrxa3TSc8WB",
        "outputId": "e7838062-7db2-4f58-e3a9-828511b1d5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss= 0.31100741028785706\n",
            "accuracy= 0.9160000085830688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Teste 25x25\n"
      ],
      "metadata": {
        "id": "mKQ_vPL9h054"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teste25x25 = Input(shape=(inpx))\n",
        "layer1 = Conv2D(32, kernel_size=(7, 7), activation='relu')(teste25x25)\n",
        "flattened = Flatten()(layer1)  # Add this line to flatten the output of the Conv2D layer\n",
        "layer2 = Dense(250, activation='sigmoid')(flattened)\n",
        "layer3 = Dense(10, activation='softmax')(layer2)"
      ],
      "metadata": {
        "id": "htmll8lTh3l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model25x25 = Model([teste25x25], layer3)\n",
        "model25x25.compile(optimizer=keras.optimizers.Adadelta(),\n",
        "\t\t\tloss=keras.losses.categorical_crossentropy,\n",
        "\t\t\tmetrics=['accuracy'])\n",
        "\n",
        "model25x25.fit(x_train, y_train, epochs=240, batch_size=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oU2jEwxh-IU",
        "outputId": "73321b57-6462-423d-f86e-16323ac9f94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/240\n",
            "120/120 [==============================] - 2s 8ms/step - loss: 2.3554 - accuracy: 0.1022\n",
            "Epoch 2/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.3330 - accuracy: 0.1023\n",
            "Epoch 3/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.3110 - accuracy: 0.1029\n",
            "Epoch 4/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2890 - accuracy: 0.1071\n",
            "Epoch 5/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2665 - accuracy: 0.1165\n",
            "Epoch 6/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2432 - accuracy: 0.1331\n",
            "Epoch 7/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.2190 - accuracy: 0.1779\n",
            "Epoch 8/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.1936 - accuracy: 0.2792\n",
            "Epoch 9/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.1667 - accuracy: 0.4123\n",
            "Epoch 10/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.1383 - accuracy: 0.5230\n",
            "Epoch 11/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.1077 - accuracy: 0.5944\n",
            "Epoch 12/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.0748 - accuracy: 0.6363\n",
            "Epoch 13/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.0391 - accuracy: 0.6616\n",
            "Epoch 14/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 2.0005 - accuracy: 0.6781\n",
            "Epoch 15/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.9586 - accuracy: 0.6927\n",
            "Epoch 16/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.9135 - accuracy: 0.7054\n",
            "Epoch 17/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.8651 - accuracy: 0.7189\n",
            "Epoch 18/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.8138 - accuracy: 0.7291\n",
            "Epoch 19/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.7599 - accuracy: 0.7378\n",
            "Epoch 20/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.7039 - accuracy: 0.7458\n",
            "Epoch 21/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.6465 - accuracy: 0.7521\n",
            "Epoch 22/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.5884 - accuracy: 0.7553\n",
            "Epoch 23/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.5301 - accuracy: 0.7603\n",
            "Epoch 24/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.4727 - accuracy: 0.7640\n",
            "Epoch 25/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.4166 - accuracy: 0.7684\n",
            "Epoch 26/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.3624 - accuracy: 0.7730\n",
            "Epoch 27/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.3106 - accuracy: 0.7765\n",
            "Epoch 28/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.2615 - accuracy: 0.7803\n",
            "Epoch 29/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.2152 - accuracy: 0.7841\n",
            "Epoch 30/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.1717 - accuracy: 0.7882\n",
            "Epoch 31/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.1311 - accuracy: 0.7922\n",
            "Epoch 32/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.0933 - accuracy: 0.7958\n",
            "Epoch 33/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.0581 - accuracy: 0.8002\n",
            "Epoch 34/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.0254 - accuracy: 0.8040\n",
            "Epoch 35/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.9951 - accuracy: 0.8084\n",
            "Epoch 36/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.9669 - accuracy: 0.8110\n",
            "Epoch 37/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.9408 - accuracy: 0.8142\n",
            "Epoch 38/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.9164 - accuracy: 0.8171\n",
            "Epoch 39/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.8937 - accuracy: 0.8195\n",
            "Epoch 40/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8725 - accuracy: 0.8220\n",
            "Epoch 41/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8527 - accuracy: 0.8244\n",
            "Epoch 42/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8341 - accuracy: 0.8268\n",
            "Epoch 43/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8167 - accuracy: 0.8289\n",
            "Epoch 44/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.8004 - accuracy: 0.8313\n",
            "Epoch 45/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7850 - accuracy: 0.8333\n",
            "Epoch 46/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7705 - accuracy: 0.8352\n",
            "Epoch 47/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7568 - accuracy: 0.8370\n",
            "Epoch 48/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7438 - accuracy: 0.8387\n",
            "Epoch 49/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7315 - accuracy: 0.8403\n",
            "Epoch 50/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7199 - accuracy: 0.8424\n",
            "Epoch 51/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.7088 - accuracy: 0.8438\n",
            "Epoch 52/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6983 - accuracy: 0.8454\n",
            "Epoch 53/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6883 - accuracy: 0.8468\n",
            "Epoch 54/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6787 - accuracy: 0.8481\n",
            "Epoch 55/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6696 - accuracy: 0.8496\n",
            "Epoch 56/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6609 - accuracy: 0.8508\n",
            "Epoch 57/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6526 - accuracy: 0.8524\n",
            "Epoch 58/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6446 - accuracy: 0.8537\n",
            "Epoch 59/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6369 - accuracy: 0.8550\n",
            "Epoch 60/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6296 - accuracy: 0.8558\n",
            "Epoch 61/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6226 - accuracy: 0.8573\n",
            "Epoch 62/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6158 - accuracy: 0.8581\n",
            "Epoch 63/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6093 - accuracy: 0.8595\n",
            "Epoch 64/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.6030 - accuracy: 0.8601\n",
            "Epoch 65/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5970 - accuracy: 0.8611\n",
            "Epoch 66/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5911 - accuracy: 0.8616\n",
            "Epoch 67/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5855 - accuracy: 0.8630\n",
            "Epoch 68/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5801 - accuracy: 0.8635\n",
            "Epoch 69/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5748 - accuracy: 0.8646\n",
            "Epoch 70/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5698 - accuracy: 0.8652\n",
            "Epoch 71/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5649 - accuracy: 0.8663\n",
            "Epoch 72/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5601 - accuracy: 0.8671\n",
            "Epoch 73/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5555 - accuracy: 0.8679\n",
            "Epoch 74/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5511 - accuracy: 0.8685\n",
            "Epoch 75/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5467 - accuracy: 0.8692\n",
            "Epoch 76/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5425 - accuracy: 0.8699\n",
            "Epoch 77/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5385 - accuracy: 0.8703\n",
            "Epoch 78/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5345 - accuracy: 0.8711\n",
            "Epoch 79/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5306 - accuracy: 0.8716\n",
            "Epoch 80/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5269 - accuracy: 0.8723\n",
            "Epoch 81/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5233 - accuracy: 0.8727\n",
            "Epoch 82/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5197 - accuracy: 0.8735\n",
            "Epoch 83/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5163 - accuracy: 0.8736\n",
            "Epoch 84/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5129 - accuracy: 0.8743\n",
            "Epoch 85/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5096 - accuracy: 0.8749\n",
            "Epoch 86/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5064 - accuracy: 0.8753\n",
            "Epoch 87/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5033 - accuracy: 0.8760\n",
            "Epoch 88/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5003 - accuracy: 0.8763\n",
            "Epoch 89/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4973 - accuracy: 0.8767\n",
            "Epoch 90/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4944 - accuracy: 0.8770\n",
            "Epoch 91/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4916 - accuracy: 0.8775\n",
            "Epoch 92/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4888 - accuracy: 0.8781\n",
            "Epoch 93/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4861 - accuracy: 0.8784\n",
            "Epoch 94/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4834 - accuracy: 0.8790\n",
            "Epoch 95/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4808 - accuracy: 0.8795\n",
            "Epoch 96/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4783 - accuracy: 0.8800\n",
            "Epoch 97/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4758 - accuracy: 0.8805\n",
            "Epoch 98/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4734 - accuracy: 0.8804\n",
            "Epoch 99/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4710 - accuracy: 0.8815\n",
            "Epoch 100/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4687 - accuracy: 0.8818\n",
            "Epoch 101/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4664 - accuracy: 0.8823\n",
            "Epoch 102/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4642 - accuracy: 0.8827\n",
            "Epoch 103/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4620 - accuracy: 0.8830\n",
            "Epoch 104/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4599 - accuracy: 0.8834\n",
            "Epoch 105/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4578 - accuracy: 0.8837\n",
            "Epoch 106/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4557 - accuracy: 0.8841\n",
            "Epoch 107/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4537 - accuracy: 0.8843\n",
            "Epoch 108/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4517 - accuracy: 0.8847\n",
            "Epoch 109/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4497 - accuracy: 0.8852\n",
            "Epoch 110/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4478 - accuracy: 0.8854\n",
            "Epoch 111/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4459 - accuracy: 0.8859\n",
            "Epoch 112/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4441 - accuracy: 0.8862\n",
            "Epoch 113/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4422 - accuracy: 0.8867\n",
            "Epoch 114/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4404 - accuracy: 0.8868\n",
            "Epoch 115/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4387 - accuracy: 0.8872\n",
            "Epoch 116/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4370 - accuracy: 0.8874\n",
            "Epoch 117/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4352 - accuracy: 0.8877\n",
            "Epoch 118/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4336 - accuracy: 0.8880\n",
            "Epoch 119/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4319 - accuracy: 0.8885\n",
            "Epoch 120/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4303 - accuracy: 0.8888\n",
            "Epoch 121/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4287 - accuracy: 0.8890\n",
            "Epoch 122/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4271 - accuracy: 0.8893\n",
            "Epoch 123/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4255 - accuracy: 0.8895\n",
            "Epoch 124/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4240 - accuracy: 0.8899\n",
            "Epoch 125/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4225 - accuracy: 0.8900\n",
            "Epoch 126/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4210 - accuracy: 0.8903\n",
            "Epoch 127/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4196 - accuracy: 0.8906\n",
            "Epoch 128/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4181 - accuracy: 0.8913\n",
            "Epoch 129/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4167 - accuracy: 0.8913\n",
            "Epoch 130/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4153 - accuracy: 0.8917\n",
            "Epoch 131/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4139 - accuracy: 0.8920\n",
            "Epoch 132/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4125 - accuracy: 0.8922\n",
            "Epoch 133/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4112 - accuracy: 0.8925\n",
            "Epoch 134/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4099 - accuracy: 0.8929\n",
            "Epoch 135/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4085 - accuracy: 0.8933\n",
            "Epoch 136/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4072 - accuracy: 0.8935\n",
            "Epoch 137/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4060 - accuracy: 0.8939\n",
            "Epoch 138/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4047 - accuracy: 0.8939\n",
            "Epoch 139/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4035 - accuracy: 0.8943\n",
            "Epoch 140/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4022 - accuracy: 0.8946\n",
            "Epoch 141/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4010 - accuracy: 0.8947\n",
            "Epoch 142/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3998 - accuracy: 0.8951\n",
            "Epoch 143/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3986 - accuracy: 0.8952\n",
            "Epoch 144/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3975 - accuracy: 0.8956\n",
            "Epoch 145/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3963 - accuracy: 0.8957\n",
            "Epoch 146/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3952 - accuracy: 0.8960\n",
            "Epoch 147/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3940 - accuracy: 0.8960\n",
            "Epoch 148/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3929 - accuracy: 0.8965\n",
            "Epoch 149/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3918 - accuracy: 0.8964\n",
            "Epoch 150/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3907 - accuracy: 0.8967\n",
            "Epoch 151/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3897 - accuracy: 0.8970\n",
            "Epoch 152/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3886 - accuracy: 0.8972\n",
            "Epoch 153/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3875 - accuracy: 0.8975\n",
            "Epoch 154/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3865 - accuracy: 0.8978\n",
            "Epoch 155/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3854 - accuracy: 0.8980\n",
            "Epoch 156/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3844 - accuracy: 0.8983\n",
            "Epoch 157/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3834 - accuracy: 0.8984\n",
            "Epoch 158/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3824 - accuracy: 0.8984\n",
            "Epoch 159/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3814 - accuracy: 0.8988\n",
            "Epoch 160/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3804 - accuracy: 0.8988\n",
            "Epoch 161/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3795 - accuracy: 0.8990\n",
            "Epoch 162/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3785 - accuracy: 0.8993\n",
            "Epoch 163/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3775 - accuracy: 0.8995\n",
            "Epoch 164/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3766 - accuracy: 0.8996\n",
            "Epoch 165/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3757 - accuracy: 0.8998\n",
            "Epoch 166/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3747 - accuracy: 0.9000\n",
            "Epoch 167/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3738 - accuracy: 0.9001\n",
            "Epoch 168/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3729 - accuracy: 0.9004\n",
            "Epoch 169/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3720 - accuracy: 0.9006\n",
            "Epoch 170/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3711 - accuracy: 0.9007\n",
            "Epoch 171/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3702 - accuracy: 0.9009\n",
            "Epoch 172/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3694 - accuracy: 0.9010\n",
            "Epoch 173/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3685 - accuracy: 0.9012\n",
            "Epoch 174/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3677 - accuracy: 0.9015\n",
            "Epoch 175/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3668 - accuracy: 0.9016\n",
            "Epoch 176/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3660 - accuracy: 0.9017\n",
            "Epoch 177/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3651 - accuracy: 0.9020\n",
            "Epoch 178/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3643 - accuracy: 0.9020\n",
            "Epoch 179/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3635 - accuracy: 0.9021\n",
            "Epoch 180/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3627 - accuracy: 0.9022\n",
            "Epoch 181/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3619 - accuracy: 0.9025\n",
            "Epoch 182/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3611 - accuracy: 0.9026\n",
            "Epoch 183/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3603 - accuracy: 0.9027\n",
            "Epoch 184/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3595 - accuracy: 0.9029\n",
            "Epoch 185/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3587 - accuracy: 0.9030\n",
            "Epoch 186/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3579 - accuracy: 0.9031\n",
            "Epoch 187/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3571 - accuracy: 0.9034\n",
            "Epoch 188/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3564 - accuracy: 0.9035\n",
            "Epoch 189/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3556 - accuracy: 0.9037\n",
            "Epoch 190/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3549 - accuracy: 0.9038\n",
            "Epoch 191/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3541 - accuracy: 0.9041\n",
            "Epoch 192/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3534 - accuracy: 0.9043\n",
            "Epoch 193/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3527 - accuracy: 0.9044\n",
            "Epoch 194/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3519 - accuracy: 0.9044\n",
            "Epoch 195/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3512 - accuracy: 0.9047\n",
            "Epoch 196/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3505 - accuracy: 0.9046\n",
            "Epoch 197/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3498 - accuracy: 0.9049\n",
            "Epoch 198/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3491 - accuracy: 0.9051\n",
            "Epoch 199/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3484 - accuracy: 0.9053\n",
            "Epoch 200/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3477 - accuracy: 0.9052\n",
            "Epoch 201/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3470 - accuracy: 0.9054\n",
            "Epoch 202/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3463 - accuracy: 0.9056\n",
            "Epoch 203/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3456 - accuracy: 0.9057\n",
            "Epoch 204/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3449 - accuracy: 0.9057\n",
            "Epoch 205/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3443 - accuracy: 0.9058\n",
            "Epoch 206/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3436 - accuracy: 0.9061\n",
            "Epoch 207/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3429 - accuracy: 0.9062\n",
            "Epoch 208/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3423 - accuracy: 0.9064\n",
            "Epoch 209/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3416 - accuracy: 0.9064\n",
            "Epoch 210/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3410 - accuracy: 0.9068\n",
            "Epoch 211/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3403 - accuracy: 0.9068\n",
            "Epoch 212/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3397 - accuracy: 0.9071\n",
            "Epoch 213/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3391 - accuracy: 0.9072\n",
            "Epoch 214/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3384 - accuracy: 0.9073\n",
            "Epoch 215/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3378 - accuracy: 0.9075\n",
            "Epoch 216/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3372 - accuracy: 0.9078\n",
            "Epoch 217/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3366 - accuracy: 0.9077\n",
            "Epoch 218/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3359 - accuracy: 0.9080\n",
            "Epoch 219/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3353 - accuracy: 0.9082\n",
            "Epoch 220/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3347 - accuracy: 0.9082\n",
            "Epoch 221/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3341 - accuracy: 0.9084\n",
            "Epoch 222/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3335 - accuracy: 0.9085\n",
            "Epoch 223/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3329 - accuracy: 0.9087\n",
            "Epoch 224/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3323 - accuracy: 0.9089\n",
            "Epoch 225/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3317 - accuracy: 0.9090\n",
            "Epoch 226/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3312 - accuracy: 0.9093\n",
            "Epoch 227/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3306 - accuracy: 0.9093\n",
            "Epoch 228/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3300 - accuracy: 0.9094\n",
            "Epoch 229/240\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3294 - accuracy: 0.9094\n",
            "Epoch 230/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3288 - accuracy: 0.9097\n",
            "Epoch 231/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3283 - accuracy: 0.9098\n",
            "Epoch 232/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3277 - accuracy: 0.9098\n",
            "Epoch 233/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3271 - accuracy: 0.9099\n",
            "Epoch 234/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3266 - accuracy: 0.9101\n",
            "Epoch 235/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3260 - accuracy: 0.9103\n",
            "Epoch 236/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3255 - accuracy: 0.9103\n",
            "Epoch 237/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3249 - accuracy: 0.9104\n",
            "Epoch 238/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3244 - accuracy: 0.9106\n",
            "Epoch 239/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3238 - accuracy: 0.9107\n",
            "Epoch 240/240\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3233 - accuracy: 0.9108\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7962b4054eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats(model25x25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcJkPqD5iAhJ",
        "outputId": "0cd16c1b-1b9d-40f3-8da8-6ac58197d246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss= 0.3098340928554535\n",
            "accuracy= 0.916700005531311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados parciais foram inconclusivos com apenas uma camada de convolução\n",
        "\n",
        "- 3x3:"
      ],
      "metadata": {
        "id": "EQB786z1kN6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "3wH0osR_GOKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inpx = Input(shape=inpx)\n",
        "layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx)\n",
        "layer2 = Conv2D(64, (3, 3), activation='relu')(layer1)\n",
        "layer3 = MaxPooling2D(pool_size=(3, 3))(layer2)\n",
        "layer4 = Dropout(0.5)(layer3)\n",
        "layer5 = Flatten()(layer4)\n",
        "layer6 = Dense(250, activation='sigmoid')(layer5)\n",
        "layer7 = Dense(10, activation='softmax')(layer6)"
      ],
      "metadata": {
        "id": "REvQX7aH342q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([inpx], layer7)\n",
        "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
        "\t\t\tloss=keras.losses.categorical_crossentropy,\n",
        "\t\t\tmetrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=120, batch_size=500)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqBU8U7hy9zD",
        "outputId": "ac6180c8-f6c4-40ce-d63f-9cf5269d3540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "120/120 [==============================] - 4s 23ms/step - loss: 2.2919 - accuracy: 0.1497\n",
            "Epoch 2/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.2853 - accuracy: 0.1612\n",
            "Epoch 3/120\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 2.2793 - accuracy: 0.1734\n",
            "Epoch 4/120\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 2.2736 - accuracy: 0.1892\n",
            "Epoch 5/120\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 2.2672 - accuracy: 0.2094\n",
            "Epoch 6/120\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 2.2618 - accuracy: 0.2250\n",
            "Epoch 7/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.2555 - accuracy: 0.2465\n",
            "Epoch 8/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.2501 - accuracy: 0.2639\n",
            "Epoch 9/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.2440 - accuracy: 0.2841\n",
            "Epoch 10/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.2379 - accuracy: 0.3075\n",
            "Epoch 11/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.2314 - accuracy: 0.3354\n",
            "Epoch 12/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.2252 - accuracy: 0.3593\n",
            "Epoch 13/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.2183 - accuracy: 0.3914\n",
            "Epoch 14/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.2108 - accuracy: 0.4173\n",
            "Epoch 15/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.2035 - accuracy: 0.4390\n",
            "Epoch 16/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.1952 - accuracy: 0.4580\n",
            "Epoch 17/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.1871 - accuracy: 0.4727\n",
            "Epoch 18/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.1776 - accuracy: 0.4870\n",
            "Epoch 19/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.1679 - accuracy: 0.4983\n",
            "Epoch 20/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.1575 - accuracy: 0.5074\n",
            "Epoch 21/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.1466 - accuracy: 0.5190\n",
            "Epoch 22/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.1332 - accuracy: 0.5304\n",
            "Epoch 23/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.1207 - accuracy: 0.5411\n",
            "Epoch 24/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.1063 - accuracy: 0.5468\n",
            "Epoch 25/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.0910 - accuracy: 0.5606\n",
            "Epoch 26/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.0752 - accuracy: 0.5638\n",
            "Epoch 27/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.0567 - accuracy: 0.5763\n",
            "Epoch 28/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 2.0379 - accuracy: 0.5817\n",
            "Epoch 29/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 2.0179 - accuracy: 0.5910\n",
            "Epoch 30/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.9963 - accuracy: 0.5967\n",
            "Epoch 31/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.9733 - accuracy: 0.6051\n",
            "Epoch 32/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.9483 - accuracy: 0.6162\n",
            "Epoch 33/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.9218 - accuracy: 0.6191\n",
            "Epoch 34/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.8946 - accuracy: 0.6301\n",
            "Epoch 35/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.8646 - accuracy: 0.6353\n",
            "Epoch 36/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.8342 - accuracy: 0.6453\n",
            "Epoch 37/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.8014 - accuracy: 0.6508\n",
            "Epoch 38/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.7678 - accuracy: 0.6586\n",
            "Epoch 39/120\n",
            "120/120 [==============================] - 3s 24ms/step - loss: 1.7332 - accuracy: 0.6625\n",
            "Epoch 40/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.6972 - accuracy: 0.6710\n",
            "Epoch 41/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.6621 - accuracy: 0.6751\n",
            "Epoch 42/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.6244 - accuracy: 0.6798\n",
            "Epoch 43/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.5875 - accuracy: 0.6865\n",
            "Epoch 44/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.5504 - accuracy: 0.6913\n",
            "Epoch 45/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.5136 - accuracy: 0.6951\n",
            "Epoch 46/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.4768 - accuracy: 0.7007\n",
            "Epoch 47/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.4411 - accuracy: 0.7056\n",
            "Epoch 48/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.4073 - accuracy: 0.7082\n",
            "Epoch 49/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.3716 - accuracy: 0.7144\n",
            "Epoch 50/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.3384 - accuracy: 0.7170\n",
            "Epoch 51/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.3061 - accuracy: 0.7228\n",
            "Epoch 52/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.2766 - accuracy: 0.7250\n",
            "Epoch 53/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.2490 - accuracy: 0.7266\n",
            "Epoch 54/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.2191 - accuracy: 0.7323\n",
            "Epoch 55/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.1940 - accuracy: 0.7343\n",
            "Epoch 56/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.1674 - accuracy: 0.7359\n",
            "Epoch 57/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.1430 - accuracy: 0.7432\n",
            "Epoch 58/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.1204 - accuracy: 0.7435\n",
            "Epoch 59/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.0989 - accuracy: 0.7466\n",
            "Epoch 60/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.0789 - accuracy: 0.7489\n",
            "Epoch 61/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.0607 - accuracy: 0.7493\n",
            "Epoch 62/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.0405 - accuracy: 0.7548\n",
            "Epoch 63/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.0245 - accuracy: 0.7543\n",
            "Epoch 64/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.0064 - accuracy: 0.7592\n",
            "Epoch 65/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.9896 - accuracy: 0.7625\n",
            "Epoch 66/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.9753 - accuracy: 0.7628\n",
            "Epoch 67/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.9612 - accuracy: 0.7653\n",
            "Epoch 68/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.9461 - accuracy: 0.7677\n",
            "Epoch 69/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.9341 - accuracy: 0.7687\n",
            "Epoch 70/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.9213 - accuracy: 0.7725\n",
            "Epoch 71/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.9102 - accuracy: 0.7721\n",
            "Epoch 72/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.8988 - accuracy: 0.7747\n",
            "Epoch 73/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.8878 - accuracy: 0.7773\n",
            "Epoch 74/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.8766 - accuracy: 0.7773\n",
            "Epoch 75/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.8671 - accuracy: 0.7803\n",
            "Epoch 76/120\n",
            "120/120 [==============================] - 3s 24ms/step - loss: 0.8578 - accuracy: 0.7826\n",
            "Epoch 77/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.8491 - accuracy: 0.7815\n",
            "Epoch 78/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.8409 - accuracy: 0.7832\n",
            "Epoch 79/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.8312 - accuracy: 0.7856\n",
            "Epoch 80/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.8229 - accuracy: 0.7869\n",
            "Epoch 81/120\n",
            "120/120 [==============================] - 3s 24ms/step - loss: 0.8139 - accuracy: 0.7882\n",
            "Epoch 82/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.8053 - accuracy: 0.7915\n",
            "Epoch 83/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7996 - accuracy: 0.7919\n",
            "Epoch 84/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7931 - accuracy: 0.7914\n",
            "Epoch 85/120\n",
            "120/120 [==============================] - 3s 24ms/step - loss: 0.7874 - accuracy: 0.7921\n",
            "Epoch 86/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7792 - accuracy: 0.7946\n",
            "Epoch 87/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7744 - accuracy: 0.7963\n",
            "Epoch 88/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7645 - accuracy: 0.7993\n",
            "Epoch 89/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7632 - accuracy: 0.7973\n",
            "Epoch 90/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7556 - accuracy: 0.7995\n",
            "Epoch 91/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7486 - accuracy: 0.8011\n",
            "Epoch 92/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7427 - accuracy: 0.8030\n",
            "Epoch 93/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7395 - accuracy: 0.8015\n",
            "Epoch 94/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7327 - accuracy: 0.8051\n",
            "Epoch 95/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7287 - accuracy: 0.8050\n",
            "Epoch 96/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.7226 - accuracy: 0.8062\n",
            "Epoch 97/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7185 - accuracy: 0.8069\n",
            "Epoch 98/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7136 - accuracy: 0.8082\n",
            "Epoch 99/120\n",
            "120/120 [==============================] - 3s 24ms/step - loss: 0.7102 - accuracy: 0.8084\n",
            "Epoch 100/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7055 - accuracy: 0.8090\n",
            "Epoch 101/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.7011 - accuracy: 0.8100\n",
            "Epoch 102/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6956 - accuracy: 0.8119\n",
            "Epoch 103/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6899 - accuracy: 0.8129\n",
            "Epoch 104/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6873 - accuracy: 0.8112\n",
            "Epoch 105/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6850 - accuracy: 0.8140\n",
            "Epoch 106/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6813 - accuracy: 0.8139\n",
            "Epoch 107/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6757 - accuracy: 0.8171\n",
            "Epoch 108/120\n",
            "120/120 [==============================] - 3s 24ms/step - loss: 0.6720 - accuracy: 0.8181\n",
            "Epoch 109/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6673 - accuracy: 0.8169\n",
            "Epoch 110/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6659 - accuracy: 0.8187\n",
            "Epoch 111/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.6636 - accuracy: 0.8187\n",
            "Epoch 112/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6584 - accuracy: 0.8211\n",
            "Epoch 113/120\n",
            "120/120 [==============================] - 3s 24ms/step - loss: 0.6547 - accuracy: 0.8207\n",
            "Epoch 114/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6519 - accuracy: 0.8229\n",
            "Epoch 115/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6486 - accuracy: 0.8232\n",
            "Epoch 116/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.6438 - accuracy: 0.8230\n",
            "Epoch 117/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6419 - accuracy: 0.8231\n",
            "Epoch 118/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6390 - accuracy: 0.8242\n",
            "Epoch 119/120\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 0.6361 - accuracy: 0.8251\n",
            "Epoch 120/120\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 0.6336 - accuracy: 0.8240\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79803e9dd210>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('loss=', score[0])\n",
        "print('accuracy=', score[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-UiNf6Ky-kj",
        "outputId": "8d32b712-a7bb-48d5-dd6f-b2f74fdd8d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss= 0.4918977916240692\n",
            "accuracy= 0.8651000261306763\n"
          ]
        }
      ]
    }
  ]
}